<!DOCTYPE html>
<html>

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-167680016-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-167680016-1');
</script>

    <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
    <link type="text/css" href="style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap" rel="stylesheet">


    <title>
        Phillip Hintikka Kieval
    </title>
</head>

<body>
      <div id="title">
        <a href="index.html">
            <center>
                <h1>Phillip Hintikka Kieval</h1>
            </center>
        </a>
    </div>
    <div id="menu">

         <a href="home.html">
            <div class="menu-links" id="home">
                <p>About</p>
            </div>
        </a>
        
        <a href="research.html">
            <div class="menu-links" id="research">
                <p>Research</p>
            </div>
        </a>
        <a href="teaching.html">
            <div class="menu-links" id="teaching">
                <p>Teaching</p>
            </div>
        </a>
        <a href="contact.html">
            <div class="menu-links" id="contact">
                <p>Contact</p>
            </div>
        </a>
        <a href="https://docs.google.com/document/d/e/2PACX-1vTdaIKg76Qe7jljtWii2Ez_CVzYLZP4CBYtXkC015OhxraInZdoS5tOcD98YccoUFly24Egp2k06cZH/pub">
        <!-- <a href="https://share.getcloudapp.com/z8unlQGD"> -->
            <div class="menu-links" id="cv">
                <p>CV</p>
            </div>
        </a>
    </div>
    <!-- Above is the end of the menu bar div and all the links inside. 
 -->

 <section class="section1">


<div class="green">
   <table> 

    <tr>

    <td class="course-name">Research Interests</td>
    <td class="course-description">

        <p>
        My work broadly concerns philosophy of machine learning, ethics of AI, and general philosophy of science.  I am interested in how humans can best use computation to understand themselves and their world.  How can we gain scientific understanding with opaque, black-box computational methods? What ways of explaining machine learning best serve scientific and public life?  How should supposedly autonomously generated concepts inspire us to revise our own?  In answering these sorts of questions, I connect traditional topics in philosophy of science such as explanation, reference, and natural kinds with a practice-based approach to the study of methods in contemporary machine learning. By examining these epistemic and normative questions, I outline more fruitful uses of machine learning for human flourishing.  

        </p>
    </td>
    <td class="research-pics">

        

    </td>
</tr>
</table>
</div>
     <table>
        
        <tr>
            <td class="course-name"><div >Philosophy of <br> Machine Learning</div></td>
            <td class="course-description"><div>
        <p> Deep neural networks are often thought to be opaque black boxes. However, the sense in which they are opaque is philosophically interesting since every component of the system can be individually surveyed.  What would it mean for such a sytem to be transparent? Is transparency required for trust? I explore explanatory strategies for opaque machine learning both as it is used for science and as it is used in public life.  For an example of this work see my paper on <a href="#transparency">transparency</a> below.
            <br>
        I am also interested in how machine learning influences scientific and social categories. When new features crosscut our existing scientific or social categories, how do we or should we revise our understanding of the world and its contents? 
        </p>
    </div></td>

     <td class="research-pics">
<div id="neural-net" class="research-circles"></div>
     </td>
  </tr>

  <tr> 
            <td class="course-name"><div >Ethics of <br> Artificial Intelligence</div></td>
            <td class="course-description"><div>
        <p> Answers to epistemic questions about transparency and explanation are relevant to the use of algorithmic decision making in public life. I consider case studies of non-state use of automated decision-making, such as automated hiring systems and loan approval algorithms. What implications do adversarial examples and other known features of deep learning systems have for transparency and fairness? For an example of a current project, <b style="color:rgb(88, 88, 88)"> <a href='https://www.dropbox.com/s/v30qykg3lyqknrm/AlgorithmicLeviathanAbstract.pdf?dl=0'>here is an abstract.</a></b>
        <br>
        Whether or not these systems are black boxes, if they are treated as such we may come to trust them on a testimonial basis.  I explore questions of machine testimony and of appropriate trust in automated decisionmaking systems.
<!-- 
Drawing on contemporary pragmatists like Elizabeth Anderson, I propose an explanatory framework for algorithmic decisions made in public life that respects the multiplicity of explanatory purposes.  Although an applicant may first be interested in the reasons for a loan's denial, upon learning more about the system she might come to be interested in whether protected features such as race were used in the making the decision and whether the decision-making system as a whole is unjust.  A pragmatist account of meaning for purpose allows us to provide reasons that shift with the epistemic goals of the users. --> 

        </p>
    </div></td>
     <td class="research-pics">
<div id="ai-ethics-pic" class="research-circles"></div>
     </td>
  </tr>

  <tr>
            <td class="course-name"><div >General Philosophy of Science</div></td>
            <td class="course-description"><div>
        <p> Machine learning is only one species of a genus of scientific methods for finding patterns in data.  This pattern-finding capacity is often thought to support the discovery of scientific phenomena, or the recognition of patterns that reflect activity and causal processes in the world rather than noise or instrument-caused artifacts of the data.  In my work in general philosophy of science, I investigate the distinguishment of signal from noise and phenomena from artifact. 

I am also interested in the normativity of scientific beliefs.  Arguments for popular forms of scientific explanation such as mechanistic explanation implicitly rely on normative theories of epistemic reason-giving.  I am interested in borrowing tools from metaethics to examine the nature/normativity of scientific belief formation.  
 </p>
    </div></td>

     <td class="research-pics">
<div id="general-philsci" class="research-circles"></div>
     </td>
  </tr>

  

  <tr>
            <td class="course-name"><div >History of Philosophy</div></td>
            <td class="course-description" style="border-bottom:dotted 1px grey;"><div>
        <p> Google’s Ali Rahimi has called machine learning a "new alchemy": a pre-paradigmatic science whose notable successes outstrip the scientific theory meant to explain them.  Early modern "natural philosophers" like Bacon, Boyle, and du Châtelet faced a similar gap between their practical ability to predict or control and their capacity to explain those successes with existing scientific theories.  In this gap flowered an integrated pursuit of observation, experimentation, epistemology, and metaphysics.  Lessons from this period, especially the methodological pursuits of Scottish enlightenment scientists such as Joseph Black and James Hutton, inform my work. <br>

        Likewise, machine learning holds out the promise, or perhaps illusion, that our technology-enhanced capacities can outstrip the human -- that we can get outside ourselves.   My research considers how to make automated decision-making systems more fair and just while grounding them in a naturalistic understanding of human sympathy and social relationships.  This work focuses on early modern sentimentalists such as David Hume, Adam Smith, and Sophie de Grouchy.  For example, Hume's theory of justice and the caprice of power underlies my most recent manuscript: the Algorithmic Leviathan, concerning arbitrariness by automated decision-making systems.
         </p>
    </div></td>
     <td class="research-pics">
<div id="earlymodern" class="research-circles"></div>
     </td>
  </tr>

    </table>


    <table>

        <tr><td class="course-name">Papers</td>

        <td class="course-description"><p id="transparency"> 
            <h3>Transparency in Complex Computational Systems (forthcoming) <br><i>Philosophy of Science (October, 2020, Volume 87 Issue 4) </i></h3>   

Abstract: Scientists depend on complex computational systems that are often ineliminably opaque, to the detriment of our ability to give scientific explanations and detect artifacts. Some philosophers have suggested treating opaque systems instrumentally, but computer scientists developing strategies for increasing transparency are correct in finding this unsatisfying. Instead, I propose an analysis of transparency as having three forms: transparency of the algorithm, the realization of the algorithm in code, and the way that code is run on particular hardware and data. This targets the transparency most useful for a task, avoiding instrumentalism by providing partial transparency when full transparency is impossible.
        </p>

            <ul>
<li> <b style="color:rgb(88, 88, 88)"> <a href='https://www.journals.uchicago.edu/doi/pdfplus/10.1086/709729/'>Link to paper</a>  </li>
    <li> <b style="color:rgb(88, 88, 88)"> <a href='http://philsci-archive.pitt.edu/16669/'>Link to preprint</a>  </li>
</ul>


        </td>

 <td class="research-pics">
<!-- <div id="transparency-pic" class="research-circles"></div> -->
 </td>
    </tr>

    <!-- <tr><td class="course-name"></td>

        <td class="course-description"><p>
            <h3>Transparency in Complex Computational Systems (forthcoming) <i>Philosophy of Science</i></h3> 
Abstract: Scientists depend on complex computational systems that are often ineliminably opaque, to the detriment of our ability to give scientific explanations and detect artifacts. Some philosophers have suggested treating opaque systems instrumentally, but computer scientists developing strategies for increasing transparency are correct in finding this unsatisfying. Instead, I propose an analysis of transparency as having three forms: transparency of the algorithm, the realization of the algorithm in code, and the way that code is run on particular hardware and data. This targets the transparency most useful for a task, avoiding instrumentalism by providing partial transparency when full transparency is impossible.
        </p>

            <ul>
    <li>TK Link to paper</li>
    <li> <b style="color:rgb(88, 88, 88)"> <a href='http://philsci-archive.pitt.edu/16669/'>Link to preprint</a>  </li>
</ul>


        </td>

    </tr> -->

  <!--   <tr><td class="course-name">Under Review</td>

        <td class="course-description"> <h3>[Paper on signal and noise] <br> <i>R&R at BJPS</i></h3> 


            <ul>
    <li>Draft available on request.</li>
</ul>


        </td>

    </tr>
 -->


    </table>


    
 
 </section>


   

    
    
</body>

</html>