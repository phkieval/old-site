<!DOCTYPE html>
<html>

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-167680016-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-167680016-1');
</script>

    <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
    <link type="text/css" href="style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap" rel="stylesheet">


    <title>
        Phillip Hintikka Kieval
    </title>
</head>

<body>


<div id="title">
        <a href="index.html">
            <center>
                <h1>Phillip Hintikka Kieval</h1>
            </center>
        </a>
    </div>
    <div id="menu">

         <a href="index.html">
            <div class="menu-links" id="index">
                <p>About</p>
            </div>
        </a>
        
        <a href="research.html">
            <div class="menu-links" id="research">
                <p>Research</p>
            </div>
        </a>
        <a href="teaching.html">
            <div class="menu-links" id="teaching">
                <p>Teaching</p>
            </div>
        </a>
        <a href="contact.html">
            <div class="menu-links" id="contact">
                <p>Contact</p>
            </div>
        </a>
       <a target = "_blank" href="KievalCV.pdf">
        
            <div class="menu-links" id="cv">
                <p>CV</p>
            </div>
        </a>
    </div>
    
 

 <section class="section1">



   <table> 

    <tr>

    <td class="course-name">Research Interests</td>
    <td class="research-summary">

        <p>
      My primary research interests concern the philosophy of machine learning and its intersections with cognitive science, philosophy of neuroscience, philosophy of biology, general philosophy of science, ethics, and public policy. I am also interested in social and political philosophy, American pragmatism, and feminist philosophy.

        </p>
    </td>
     <td class="research-pics">
<div id="alphafold" class="research-circles"></div>
     </td>
</tr>
</table>

    

<table> 

    <tr>

    <td class="course-name">Publications</td>
    <td class="course-description">

    <p> <h3>Kieval, Phillip Hintikka (forthcoming), "Mapping representational spaces with deep neural networks" <i>Synthese.</i></h3> 


   The predominance of machine learning (ML) based techniques in cognitive neuroscience raises a host of philosophical and methodological concerns. Given the messiness of neural activity, modellers must make choices about how to structure their raw data to make inferences about encoded representations. This leads to a set of standard methodological assumptions about when abstraction is appropriate in neuroscientific practice. Yet, when made uncritically these choices threaten to bias conclusions about phenomena drawn from data. Contact between the practices of multivariate pattern analysis (MVPA) and philosophy of science can help to illuminate the conditions under which we can use artificial neural networks to better understand neural mechanisms. This paper considers a specific technique for MVPA called representational similarity analysis (RSA). I develop a theoretically-informed account of RSA that draws on early connectionist research and work on idealization in the philosophy of science. By bringing a philosophical account of cognitive modelling in conversation with RSA, this paper clarifies the practices of neuroscientists and provides a generalizable framework for using artificial neural networks to study neural mechanisms in the brain.
   
    <ul><li> Draft available upon request.</ul>



    </p>

</tr>
</table>


<table> 

    <tr>

    <td class="course-name">Works in Progress</td>
    <td class="course-description">

    <p>
        <h3>Unsupervised Discoveries, Understanding, and Semantic Opacity  (In Progress)</i></h3>   

       
       
        In this paper, I draw attention to an under-theorized problem for the application of unsupervised machine learning models in science. I call this problem \textit{semantic opacity.} Semantic opacity occurs when the knowledge needed to translate the output of an unsupervised model into scientific concepts depends on theoretical assumptions about the same domain of inquiry into which the model purports to grant insight. Semantic opacity is especially likely to occur in exploratory contexts, wherein experimentation is not strongly guided by extant theory. Exploratory experimentation is thought to provide the conditions for the development of new scientific concepts. However, when exploratory experimentation is mediated by machine learning, we lack the interpretative tools needed to decide if the predictions of a model correspond to robust, scientific kinds rather than jerry-rigged ones tied to spurious correlations in Big Data. Furthermore, I argue that techniques in explainable AI (XAI) that aim to make these models more interpretable are not well suited to address semantic opacity.  
         <ul><li> Draft available upon request.</ul>



        </p>
    </td>

   

</tr>
</table>

<table> 

    <tr>

    <td class="course-name"></td>
    <td class="course-description">

    <p>
        <h3>Biased-by-design: why algorithms are necessarily value-laden (In Progress)</i></h3>   

       Algorithmic decision-making systems applied in social contexts drape value-laden solutions in an illusory veil of objectivity. I argue that these systems are necessarily value-laden and that this follows from the need to construct a quantifiable objective function. Many researchers have convincingly argued that machine learning systems learn to replicate and amplify pre-existing biases of moral import found in training data. But these arguments permit a strategic retreat for those who nevertheless maintain that algorithms themselves are value-neutral. Proponents of the value-neutrality of algorithms argue that while the existence of algorithmic bias is undeniable such bias is merely the product of bad data curation practices. On such a view, eliminating biased data would obliterate any values embedded in algorithmic decision-making. This position can be neatly summarized by the slogan “Algorithms aren’t biased, data is biased.” However, this attitude towards algorithms is misguided. Training machine learning algorithms involves optimization, which requires either minimizing an error function or maximizing an objective function by iteratively adjusting a model’s parameters. The objective function represents the quality of the solution found by the algorithm as a single real number. Training an algorithm thus aggregates countless indicators of predictive success into a single, automatically generated, weighted index. But deciding to operationalize a particular goal in this way is itself a value-laden choice. This is because many qualities we want to predict are qualitative concepts with multifaceted meanings. Such concepts like “health” or “job-applicant-quality” lack sharp boundaries and admit plural and context-dependent meanings. Collapsing concepts into a quantifiable ratio scale of predictive success flattens out their quality dimensions. This process is often underdetermined and arbitrary, but convenient for enterprises that rely on precise and unambiguous predictions. Hence, the very choice to use an algorithm in the first place reflects the values and priorities of particular stakeholders.



        </p>
    </td>

   

</tr>
</table>

<table> 

    <tr>

    <td class="course-name"></td>
    <td class="course-description">

    <p>
        <h3>Dimensionality and the Empirical Justification of Measurement Standards in Cognitive Neuroscience  (In Progress)</i></h3>   

       
        Both artificial and biological neural networks consist in massively many parameters resulting in computations being performed in a high-dimensional space. As such, neuroscientists aiming to locate the neural basis for representations must take steps to address the curse of dimensionality---a set of problems that emerge when analyzing data in high-dimensional spaces that otherwise would not occur in low-dimensional spaces like those of ordinary three-dimensional experience. Typically, this involves various forms of dimensionality reduction, normalization, and implicit assumptions about the distribution of noise. The theoretical importance of these modelling choices is rarely discussed by the modelers themselves. This paper argues that much of the interesting work in an analysis of neural activity is actually being done by early choices that modelers make to structure their raw data. This phenomenon is especially embodied by the choice of dimensionality reduction and the selection of distance metric. Examining these practices reveals a theoretical challenge familiar to all burgeoning empirical sciences. The procedures used to generate quantitative measurements are themselves dependent on hypotheses that call out for empirical justification. Measurement outcomes depend on theoretical assumptions embedded in modeling decisions, but any forthcoming empirical justification for these choices must refer back to theories that cannot themselves be verified by observation without reference to further theoretical assumptions. Neuroscience must, therefore, abandon the demand for a "bias-free", or "hypothesis-neutral", foundation on which to ground its empiricism. Neuroscientists should instead adopt a coherentist mode of justification on which disparate modeling practices converge upon increasingly precise measurement outcomes through a process of epistemic iteration.



        </p>
    </td>

   

</tr>
</table>




<table> 

    <tr>

    <td class="course-name"></td>
    <td class="course-description">

    <p> <h3>Towards a Peircian Pragmatist Account of Representation in Cognitive Science</i></h3> 



    </p>

</tr>
</table>

  <!--<table> 

    <tr>

    <td class="course-name"> </td>
    <td class="course-description">

    <p>
        <h3>Permission to Believe at Will </i></h3>   

       According to doxastic involuntarism, we cannot believe at will. In this paper, I argue that permissivism, the view that, at times, there is more than one way to respond rationally to a given body of evidence, is consistent with doxastic involuntarism. Blake Roeber (2019a,b) argues that, since permissive situations are possible, cognitively healthy agents can believe at will. However, Roeber (2019b) fails to distinguish between two different arguments for voluntarism, both of which can be shown to fail by proper attention to different accounts of permissivism. Roeber considers a generic treatment of permissivism, but key premises in both arguments depend on different, more particular notions of permissivism. Attending to these differences reveals that the inference from permissivism to voluntarism to be unwarranted.  
     
      <ul><li>Draft available upon request</ul> 
        



        </p>
    </td>

   

</tr>
</table> -->


 

 </section>



    
    
</body>

</html>