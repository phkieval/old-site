<!DOCTYPE html>
<html>

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-167680016-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-167680016-1');
</script>

    <link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
    <link type="text/css" href="style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Caslon+Text&display=swap" rel="stylesheet">


    <title>
        Phillip Hintikka Kieval
    </title>
</head>

<body>


<div id="title">
        <a href="index.html">
            <center>
                <h1>Phillip Hintikka Kieval</h1>
            </center>
        </a>
    </div>
    <div id="menu">

         <a href="index.html">
            <div class="menu-links" id="index">
                <p>About</p>
            </div>
        </a>
        
        <a href="research.html">
            <div class="menu-links" id="research">
                <p>Research</p>
            </div>
        </a>
        <a href="teaching.html">
            <div class="menu-links" id="teaching">
                <p>Teaching</p>
            </div>
        </a>
        <a href="contact.html">
            <div class="menu-links" id="contact">
                <p>Contact</p>
            </div>
        </a>
       <a target = "_blank" href="KievalCV.pdf">
        
            <div class="menu-links" id="cv">
                <p>CV</p>
            </div>
        </a>
    </div>
    
 

 <section class="section1">



   <table> 

    <tr>

    <td class="course-name">Research Interests</td>
    <td class="research-summary">

        <p>
      My primary research interests concern the philosophy of machine learning and its intersections with cognitive science, philosophy of neuroscience, philosophy of biology, general philosophy of science, ethics, and public policy. I am also interested in social and normative epistemology, pragmatism, and feminist philosophy.

        </p>
    </td>
     <td class="research-pics">
<div id="deep-learning" class="research-circles"></div>
     </td>
</tr>
</table>

    
     

<table> 

    <tr>

        <td class="course-name"><div>Philosophy of AI</div></td>
        <td class="research-summary"><div>

        <p>
      My most recent work focuses on the structure and content of representations in deep neural networks. State-of-the-art machine learning systems are ubiquitous in modern life. Deep neural networks exhibit a remarkable level of predictive success worthy of the popular label of artificial intelligence (AI). These systems find applications in everything from automated decision assistance in medicine and criminal justice to playing board games like chess and Go. Our epistemic networks are increasingly intertwined with machine learning algorithms, and scientists rely on them in complex computational models. However, deep learning systems are opaque in ways that make explaining their capacities intractable. Philosophers of science are uniquely positioned to investigate critical questions concerning the widespread implementation of machine learning systems. How can deep neural networks teach us about the brain? How do these models generate explanations in science? To what extent does science demand transparency in AI? How should rapid technological advancements in AI inform public policy? How can we promote more humane and egalitarian implementations of AI in public life? I am optimistic that we can glean some answers to these questions by investigating how deep learning systems learn and exploit representations.

        </p>
    </td>
     <td class="research-pics">
<div id="alphafold" class="research-circles"></div>
     </td>
</tr>
</table>

<table> 

    <tr>

    <td class="course-name">Works in Progress</td>
    <td class="course-description">

    <p>
        <h3>Dimensionality and the Empirical Justification of Measurement Standards in Cognitive Neuroscience  (In Progress)</i></h3>   

       
        Both artificial and biological neural networks consist in massively many parameters resulting in computations being performed in a high-dimensional space. As such, neuroscientists aiming to locate the neural basis for representations must take steps to address the curse of dimensionality---a set of problems that emerge when analyzing data in high-dimensional spaces that otherwise would not occur in low-dimensional spaces like those of ordinary three-dimensional experience. Typically, this involves various forms of dimensionality reduction, normalization, and implicit assumptions about the distribution of noise. The theoretical importance of these modelling choices is rarely discussed by the modelers themselves. This paper argues that much of the interesting work in an analysis of neural activity is actually being done by early choices that modelers make to structure their raw data. This phenomenon is especially embodied by the choice of dimensionality reduction and the selection of distance metric. Examining these practices reveals a theoretical challenge familiar to all burgeoning empirical sciences. The procedures used to generate quantitative measurements are themselves dependent on hypotheses that call out for empirical justification. Measurement outcomes depend on theoretical assumptions embedded in modeling decisions, but any forthcoming empirical justification for these choices must refer back to theories that cannot themselves be verified by observation without reference to further theoretical assumptions. Neuroscience must, therefore, abandon the demand for a "bias-free", or "hypothesis-neutral", foundation on which to ground its empiricism. Neuroscientists should instead adopt a coherentist mode of justification on which disparate modeling practices converge upon increasingly precise measurement outcomes through a process of epistemic iteration.



        </p>
    </td>

   

</tr>
</table>

<table> 

    <tr>

    <td class="course-name">Works in Progress</td>
    <td class="course-description">

    <p>
        <h3>Unsupervised Discoveries, Understanding, and Semantic Opacity  (In Progress)</i></h3>   

       
       
        State-of-the-art machine learning (ML) models based on deep neural networks (DNNs) exhibit predictive accuracy that far outstrips that of previous hand-coded models. This success has generated widespread optimism at the prospect of using DNNs to enhance the march of scientific progress in a diverse range of fields. By using these models to explore massive collections of data, scientists might be able to produce novel discoveries with DNNs. However, DNNs are opaque in ways that preclude scientific understanding. Thus, their predictive accuracy comes at the cost of one of the central aims of scientific inquiry. This tension has brought about increased interest in explainable AI (XAI), a growing discipline that aims to understand and explain how these models work. However, I argue that XAI as it is now conceived cannot deliver on the promise of understanding in the scientific contexts in which DNNs purport to show the greatest promise, namely in exploratory contexts. Traditional mathematical and computational models typically depend on parameters whose values represent the properties of their target system. By contrast, DNNs lack any such interpretable mapping between parameters and properties of real systems. Instead, these models depend on hyperparameters that determine how a model will learn to generate input/output mappings in a way that optimizes some measure of performance. Currently, XAI aims to explain how these models optimize performance and make decisions. Yet, in exploratory contexts where scientists hope to produce discoveries, it will be the very content of a modelâ€™s output that requires understanding. In such contexts, scientists will deploy unsupervised models, which generate clustering patterns from unlabeled data. Given that the hope is to explore domains of inquiry that are not already well understood, we will lack the interpretative machinery to decide if the clusters of data produced by such a model correspond to real, scientific kinds rather than jerry-rigged ones tied to spurious correlations in big data. In exploratory contexts we lack the conceptual frameworks necessary to say what the output clusters of unsupervised learning models mean. I call this phenomenon semantic opacity. When confronted with semantic opacity, the knowledge required for interpreting the decisions of a model depends on theoretical assumptions about the very domain of inquiry about which we had hoped the model could teach us. I argue that this should be the understood as the more pressing problem facing XAI in the context of science.  




        </p>
    </td>

   

</tr>
</table>


<table> 

    <tr>

    <td class="course-name"></td>
    <td class="course-description">

    <p> <h3>Mapping representational spaces with deep neural networks (Under review)</i></h3> 


   The predominance of machine learning (ML) based techniques in cognitive neuroscience raises a host of philosophical and methodological concerns. Given the messiness of neural activity, modellers must make choices about how to structure their raw data to make inferences about encoded representations. This leads to a set of standard methodological assumptions about when abstraction is appropriate in neuroscientific practice. But when made uncritically these choices threaten to bias conclusions about phenomena drawn from data. Contact between the practices of multivariate pattern analysis (MVPA) and philosophy of science can help to illuminate the conditions under which we can use artificial neural networks to better understand neural mechanisms. This paper considers a specific technique for MVPA called representational similarity analysis (RSA). I develop a theoretically-informed account of RSA that draws on early connectionist research and work on idealization in the philosophy of science. By bringing a philosophical account of cognitive modelling in conversation with RSA, this paper clarifies the practices of neuroscientists and provides a generalizable framework for using artificial neural networks to study cognitive mechanisms in the brain.
    <ul><li> Draft available upon request.</ul>



    </p>

</tr>
</table>



  <!--<table> 

    <tr>

    <td class="course-name"> </td>
    <td class="course-description">

    <p>
        <h3>Permission to Believe at Will (Under review)</i></h3>   

       According to doxastic involuntarism, we cannot believe at will. In this paper, I argue that permissivism, the view that, at times, there is more than one way to respond rationally to a given body of evidence, is consistent with doxastic involuntarism. Blake Roeber (2019a,b) argues that, since permissive situations are possible, cognitively healthy agents can believe at will. However, Roeber (2019b) fails to distinguish between two different arguments for voluntarism, both of which can be shown to fail by proper attention to different accounts of permissivism. Roeber considers a generic treatment of permissivism, but key premises in both arguments depend on different, more particular notions of permissivism. Attending to these differences reveals that the inference from permissivism to voluntarism to be unwarranted.  
     
      <ul><li>Draft available upon request</ul> 
        



        </p>
    </td>

   

</tr>
</table> -->


 

 </section>



    
    
</body>

</html>